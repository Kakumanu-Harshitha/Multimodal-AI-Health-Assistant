# backend/llm_service.py
import os
from groq import Groq
from dotenv import load_dotenv

load_dotenv()

LLM_MODEL = "llama-3.3-70b-versatile"
client = Groq(api_key=os.getenv("GROQ_API_KEY")) if os.getenv("GROQ_API_KEY") else None
# Initialize the Groq client for the LLM
if os.getenv("GROQ_API_KEY"):
    client = Groq(api_key=os.getenv("GROQ_API_KEY"))
    print("✅ Groq client for LLM initialized.")
else:
    print("⚠️ WARNING: GROQ_API_KEY not found! LLM service disabled.")

def generate_health_report_insights(
    profile: dict,
    recent_symptoms: str,
    past_context: str,
) -> dict:
    """
    Returns:
    {
        summary: str,
        interpretation: str,
        food_recommendations: str
    }
    """
    if not client:
        return {
            "summary": "LLM service unavailable.",
            "interpretation": "LLM service unavailable.",
            "food_recommendations": "LLM service unavailable."
        }

    system_prompt = """
You are a clinical AI health assistant.
Generate a SHORT, PROFESSIONAL health report.

Rules:
- No long explanations
- No conversation tone
- No repeated disclaimers
- Food advice must be PERSONALIZED
- Output must be concise and structured
"""

    user_prompt = f"""
User Profile:
Age: {profile.get('age')}
Gender: {profile.get('gender')}
Allergies: {profile.get('allergies')}
Chronic Conditions: {profile.get('chronic_diseases')}

Recent Symptoms:
{recent_symptoms}

Past Health Context:
{past_context}

Generate:
1. Health Summary (4–5 lines)
2. Medical Interpretation (3–4 lines)
3. Personalized Food & Lifestyle Advice (bullet points)
"""

    response = client.chat.completions.create(
        model=LLM_MODEL,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
    )

    content = response.choices[0].message.content.strip()

    # Simple structured split
    sections = content.split("\n\n")

    return {
        "summary": sections[0] if len(sections) > 0 else "",
        "interpretation": sections[1] if len(sections) > 1 else "",
        "food_recommendations": sections[2] if len(sections) > 2 else "",
    }


def get_llm_response(prompt: str, conversation_history: list = None) -> str:
    """Generates a structured, safe medical response from the LLM."""
    if not client:
        return "LLM service is unavailable — please check the GROQ_API_KEY in your .env file."

    # This system prompt guides the AI to provide safe and responsible answers
    system_prompt = (
        "You are a highly sophisticated and empathetic AI Health Assistant. "
        "Your primary role is to provide safe, informative, and helpful preliminary guidance based on user-provided symptoms, medical questions, or images. "
        "You must adhere to the following strict guidelines for every response:\n\n"
        "1. **Safety First Disclaimer (Mandatory):** ALWAYS begin your response with a clear and prominent disclaimer. State that you are an AI assistant, not a medical professional, and your analysis is for informational purposes only. Strongly urge the user to consult a qualified healthcare provider for an accurate diagnosis and treatment plan.\n\n"
        "2. **Symptom Analysis:** Carefully analyze the symptoms or query provided by the user.\n\n"
        "3. **Provide Potential Conditions:** Based on the analysis, list a few *potential* conditions that might be associated with the symptoms. Use cautious language like 'Some conditions that can cause these symptoms include...' or 'This could possibly be related to...'.\n\n"
        "4. **Actionable Advice & Recommendations:** Provide general, safe, and actionable advice. This should include lifestyle or dietary suggestions where appropriate.\n\n"
        "5. **NEVER Diagnose:** Under no circumstances should you provide a definitive diagnosis. Do not say 'You have...' or 'This is...'. Always frame it as a possibility.\n\n"
        "6. **Empathetic Tone:** Maintain a professional, calm, and empathetic tone throughout the conversation."
    )
    
    messages = [{"role": "system", "content": system_prompt}]
    if conversation_history:
        messages.extend(conversation_history)
    messages.append({"role": "user", "content": prompt})
    
    try:
        chat_completion = client.chat.completions.create(
            messages=messages,
            model=LLM_MODEL,
        )
        return chat_completion.choices[0].message.content
    except Exception as e:
        print(f"❌ ERROR: Groq LLM API call failed. Error: {e}")
        return "I'm sorry, I encountered an error while processing your request."